{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS5785 Fall 2021 Applied Machine Learning Homework 2: PROGRAMMING EXERCISE 2 - Sentiment analysis of online reviews\n",
    "\n",
    "### By Hao Geng (hg457),  Siyi Chen(sc2358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.naive_bayes import GaussianNB #高斯分布型\n",
    "# from sklearn.naive_bayes import BernoulliNB #伯努利型\n",
    "# from sklearn.naive_bayes import MultinomialNB #多项式型\n",
    "from sklearn import linear_model\n",
    "# from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). There are three data ﬁles under the root folder. yelp_labelled.txt, amazon_cells_labelled.txt and imdb_labelled.txt. Parse each ﬁle with the speciﬁcations in readme.txt. Are the labels balanced? If not, what’s the ratio between the two labels? Explain how you process these ﬁles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(filename):\n",
    "  return pandas.read_csv('amazon_cells_labelled.txt', sep = '\\t', \n",
    "                        header=None, names = ['sentences','scores'])\n",
    "\n",
    "df_amazon = read_df('amazon_cells_labelled.txt')\n",
    "df_imdb = read_df('imdb_labelled.txt')\n",
    "df_yelp = read_df('yelp_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(sum(df_amazon['scores']))\n",
    "print(sum(df_imdb['scores']))\n",
    "print(sum(df_yelp['scores']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "Each dataframe has 500 negative and positive labels respectively. So they are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) *Pick your preprocessing strategy*. Since these sentences are online reviews, they may contain signiﬁcant amounts of noise and garbage. You *may or may not* want to do one or all of the following. Explain the reasons for each of your decision (*why or why not*).\n",
    "- Lower case all of the words.\n",
    "- Lemmatization of all the words (i.e.,convert every word to its root so that all of “running,” “run,” and “runs” are converted to “run” and all of“good,” “well,” “better,” and “best” are converted to “good”; this is easily done using [nltk.stem](http://www.nltk.org/api/nltk.stem.html)).\n",
    "- Strip punctuation.\n",
    "- Strip the stop words,e.g., “the”, “and”, “or”.\n",
    "- Something else? Tell us about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Lower case all of the words.\n",
    "    - Most time words mean the same no matter their cases. Lowering case them can reduce the word count (feature count) and unify the intepretations of words with different cases. In this task, lower cases would be good.\n",
    "- Lemmatization of all the words (i.e.,convert every word to its root so that all of “running,” “run,” and “runs” are converted to “run” and all of“good,” “well,” “better,” and “best” are converted to “good”; this is easily done using [nltk.stem](http://www.nltk.org/api/nltk.stem.html)).\n",
    "    - Most every (content) word in English can take on several forms. Sometimes these changes are meaningful, and sometimes they are just to serve a certain grammatical context. Lemmatization words helps extricate high-quality information from text so that all variants are consistent across documents. But Lemmatization can lose accuracy, like 'best' should have a higher indication then 'good' for positive reviews. Not lemmatizing is the conservative approach, and should be favored unless there is a significant performance gain. In this task, lemmatize words would be good.\n",
    "- Strip punctuation.\n",
    "- Strip the stop words,e.g., “the”, “and”, “or”.\n",
    "    - Stop words and punctuations are the most frequent words in any slice of text, and mostly seemingly contentless. But they may be relevant to the meaning and function of similar words in rare cases. And these stop words and punctuations may indicate the length of the text. Considering the number of them is limited, this task would not eliminate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Split training and testing set. In this assignment, for each ﬁle, please use the ﬁrst 400 instances for each label as the training set and the remaining 100 instances as testing set. In total,there are 2400 reviews for training and 600 reviews for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_4_1(df_amazon, negative_index):\n",
    "  df_amazon_positive = df_amazon[df_amazon['scores'] == 1]\n",
    "  df_amazon_negative = df_amazon[df_amazon['scores'] == 0]\n",
    "\n",
    "  df_amazon_train = df_amazon_positive[:negative_index].append(df_amazon_negative[:negative_index]).reset_index()\n",
    "  df_amazon_test = df_amazon_positive[negative_index:].append(df_amazon_negative[negative_index:]).reset_index()\n",
    "\n",
    "  return df_amazon_train, df_amazon_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and testing set respectively\n",
    "df_amazon_train, df_amazon_test = split_df_4_1(df_amazon, 400)\n",
    "df_imdb_train, df_imdb_test = split_df_4_1(df_imdb, 400)\n",
    "df_yelp_train, df_yelp_test = split_df_4_1(df_yelp, 400)\n",
    "\n",
    "# combine the splited sets to a whole set\n",
    "df_all_train = df_amazon_train.append(df_imdb_train).append(df_yelp_train).reset_index()\n",
    "df_all_test = df_amazon_test.append(df_imdb_test).append(df_yelp_test).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>sentences</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>If you are Razr owner...you must have this!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>And the sound quality is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>795</td>\n",
       "      <td>814</td>\n",
       "      <td>Battery has no life.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>796</td>\n",
       "      <td>815</td>\n",
       "      <td>I checked everywhere and there is no feature f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>797</td>\n",
       "      <td>818</td>\n",
       "      <td>Doesn't do the job.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>798</td>\n",
       "      <td>824</td>\n",
       "      <td>Awkward to use and unreliable.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>799</td>\n",
       "      <td>826</td>\n",
       "      <td>Not as good as I had hoped.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      level_0  index                                          sentences  \\\n",
       "0           0      1                        Good case, Excellent value.   \n",
       "1           1      2                             Great for the jawbone.   \n",
       "2           2      4                                  The mic is great.   \n",
       "3           3      7        If you are Razr owner...you must have this!   \n",
       "4           4     10                    And the sound quality is great.   \n",
       "...       ...    ...                                                ...   \n",
       "2395      795    814                               Battery has no life.   \n",
       "2396      796    815  I checked everywhere and there is no feature f...   \n",
       "2397      797    818                                Doesn't do the job.   \n",
       "2398      798    824                     Awkward to use and unreliable.   \n",
       "2399      799    826                        Not as good as I had hoped.   \n",
       "\n",
       "      scores  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "2395       0  \n",
       "2396       0  \n",
       "2397       0  \n",
       "2398       0  \n",
       "2399       0  \n",
       "\n",
       "[2400 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>sentences</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>778</td>\n",
       "      <td>This is a great deal.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>787</td>\n",
       "      <td>It is simple to use and I like it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>788</td>\n",
       "      <td>It's a great tool for entertainment, communica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>791</td>\n",
       "      <td>I own 2 of these cases and would order another.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>792</td>\n",
       "      <td>Great Phone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>195</td>\n",
       "      <td>995</td>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>196</td>\n",
       "      <td>996</td>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>197</td>\n",
       "      <td>997</td>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>198</td>\n",
       "      <td>998</td>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>199</td>\n",
       "      <td>999</td>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0  index                                          sentences  scores\n",
       "0          0    778                              This is a great deal.       1\n",
       "1          1    787                 It is simple to use and I like it.       1\n",
       "2          2    788  It's a great tool for entertainment, communica...       1\n",
       "3          3    791    I own 2 of these cases and would order another.       1\n",
       "4          4    792                                       Great Phone.       1\n",
       "..       ...    ...                                                ...     ...\n",
       "595      195    995  The screen does get smudged easily because it ...       0\n",
       "596      196    996  What a piece of junk.. I lose more calls on th...       0\n",
       "597      197    997                       Item Does Not Match Picture.       0\n",
       "598      198    998  The only thing that disappoint me is the infra...       0\n",
       "599      199    999  You can not answer calls with the unit, never ...       0\n",
       "\n",
       "[600 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Bag of Words model. Extract features and then represent each review using bag of words model, i.e., every word in the review becomes its own element in a feature vector. In orderto do this, ﬁrst, make one pass through all the reviews in the training set (Explain why we can’t use testing set at this point) and build a dictionary of unique words. Then, make another pass through the review in both the training set and testing set and count up the occurrences of each word in your dictionary. The ith element of a review’s feature vector is the number of occurrences of the ith dictionary word in the review. Implement the bag of words model and report feature vectors of any two reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the nltk downloader\n",
    "# nltk.download() # Click on Models tab and select punkt and click Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemed_word_df(df):\n",
    "\n",
    "    df_words = pandas.DataFrame()\n",
    "\n",
    "    for sentence in df['sentences']:\n",
    "        token_dict = {}\n",
    "\n",
    "        token_words=word_tokenize(sentence)\n",
    "        for word in token_words:\n",
    "            stemed = porter.stem(word)\n",
    "            token_dict[stemed] = token_dict.get(stemed, 0) + 1\n",
    "    #     print(token_dict)\n",
    "\n",
    "        df_words = df_words.append(token_dict, ignore_index=True)\n",
    "        \n",
    "    return df_words.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of training data with words as features\n",
    "df_all_train_words = get_stemed_word_df(df_all_train)\n",
    "dict_all_train_words = df_all_train_words.to_dict()\n",
    "matrix_all_train = np.array(df_all_train_words)\n",
    "\n",
    "# df of test data with words as features\n",
    "df_all_test_words = get_stemed_word_df(df_all_test)\n",
    "\n",
    "# exclude the columns of features not includeed in training data\n",
    "to_drop = []\n",
    "for col in df_all_test_words.columns.values.tolist():\n",
    "    if col not in df_all_train_words.columns.values.tolist():\n",
    "        to_drop.append(col)\n",
    "\n",
    "df_all_test_words = df_all_test_words.drop(columns = to_drop)\n",
    "matrix_all_test = np.array(df_all_test_words)\n",
    "\n",
    "# df of all data with words as features, combine training and test data\n",
    "df_all_words = df_all_train_words.append(df_all_test_words).fillna(0)\n",
    "dict_all_words = df_all_words.to_dict()\n",
    "matrix_all = np.array(df_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>case</th>\n",
       "      <th>excel</th>\n",
       "      <th>good</th>\n",
       "      <th>valu</th>\n",
       "      <th>for</th>\n",
       "      <th>great</th>\n",
       "      <th>jawbon</th>\n",
       "      <th>the</th>\n",
       "      <th>...</th>\n",
       "      <th>shout</th>\n",
       "      <th>telephon</th>\n",
       "      <th>wind</th>\n",
       "      <th>7.44</th>\n",
       "      <th>grtting</th>\n",
       "      <th>until</th>\n",
       "      <th>v3c</th>\n",
       "      <th>improp</th>\n",
       "      <th>everywher</th>\n",
       "      <th>awkward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 1452 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ,    .  case  excel  good  valu  for  great  jawbon  the  ...  shout  \\\n",
       "0     1.0  1.0   1.0    1.0   1.0   1.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "1     0.0  1.0   0.0    0.0   0.0   0.0  1.0    1.0     1.0  1.0  ...    0.0   \n",
       "2     0.0  1.0   0.0    0.0   0.0   0.0  0.0    1.0     0.0  1.0  ...    0.0   \n",
       "3     0.0  0.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "4     0.0  1.0   0.0    0.0   0.0   0.0  0.0    1.0     0.0  1.0  ...    0.0   \n",
       "...   ...  ...   ...    ...   ...   ...  ...    ...     ...  ...  ...    ...   \n",
       "2395  0.0  1.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "2396  0.0  1.0   0.0    0.0   0.0   0.0  1.0    0.0     0.0  0.0  ...    0.0   \n",
       "2397  0.0  1.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  1.0  ...    0.0   \n",
       "2398  0.0  1.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "2399  0.0  1.0   0.0    0.0   1.0   0.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "\n",
       "      telephon  wind  7.44  grtting  until  v3c  improp  everywher  awkward  \n",
       "0          0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "1          0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "2          0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "3          0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "4          0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "...        ...   ...   ...      ...    ...  ...     ...        ...      ...  \n",
       "2395       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "2396       0.0   0.0   0.0      0.0    0.0  0.0     0.0        1.0      0.0  \n",
       "2397       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "2398       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      1.0  \n",
       "2399       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "\n",
       "[2400 rows x 1452 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_all_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>case</th>\n",
       "      <th>excel</th>\n",
       "      <th>good</th>\n",
       "      <th>valu</th>\n",
       "      <th>for</th>\n",
       "      <th>great</th>\n",
       "      <th>jawbon</th>\n",
       "      <th>the</th>\n",
       "      <th>...</th>\n",
       "      <th>shout</th>\n",
       "      <th>telephon</th>\n",
       "      <th>wind</th>\n",
       "      <th>7.44</th>\n",
       "      <th>grtting</th>\n",
       "      <th>until</th>\n",
       "      <th>v3c</th>\n",
       "      <th>improp</th>\n",
       "      <th>everywher</th>\n",
       "      <th>awkward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 1452 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ,    .  case  excel  good  valu  for  great  jawbon  the  ...  shout  \\\n",
       "0    1.0  1.0   1.0    1.0   1.0   1.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "1    0.0  1.0   0.0    0.0   0.0   0.0  1.0    1.0     1.0  1.0  ...    0.0   \n",
       "2    0.0  1.0   0.0    0.0   0.0   0.0  0.0    1.0     0.0  1.0  ...    0.0   \n",
       "3    0.0  0.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "4    0.0  1.0   0.0    0.0   0.0   0.0  0.0    1.0     0.0  1.0  ...    0.0   \n",
       "..   ...  ...   ...    ...   ...   ...  ...    ...     ...  ...  ...    ...   \n",
       "595  0.0  1.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  1.0  ...    0.0   \n",
       "596  0.0  1.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "597  0.0  1.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  0.0  ...    0.0   \n",
       "598  0.0  1.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  2.0  ...    0.0   \n",
       "599  1.0  0.0   0.0    0.0   0.0   0.0  0.0    0.0     0.0  1.0  ...    0.0   \n",
       "\n",
       "     telephon  wind  7.44  grtting  until  v3c  improp  everywher  awkward  \n",
       "0         0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "1         0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "2         0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "3         0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "4         0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "..        ...   ...   ...      ...    ...  ...     ...        ...      ...  \n",
       "595       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "596       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "597       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "598       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "599       0.0   0.0   0.0      0.0    0.0  0.0     0.0        0.0      0.0  \n",
       "\n",
       "[3000 rows x 1452 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- Explain why we can’t use testing set at this point\n",
    "    - We need to make one pass in the training set because we need to collect all the words(features). If we only go through the whole dataset, we will collect all the words, including those do not show up in the training dataset. Words not in the training dataset should not be included in the features since we can not draw information of their relevance to result based on training date. Any values we set to those words will be assumptions, which we should not do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Pick your postprocessing strategy. Since the vast majority of English words will not appear in most of the reviews, most of the feature vector elements will be 0. This suggests that we need a postprocessing or normalization strategy that combats the huge variance of the elements in the feature vector. You may want to use one of the following strategies. Whatever choices you make, explain why you made the decision. \n",
    "- log-normalization. For each element of the feature vector x, transform it into f(x) = log(x+1).\n",
    "- l1 normalization. Normalize the l1 norm of the feature vector, ˆ x= x / |x| .\n",
    "- l2 normalization. Normalize the l2 norm of the feature vector, ˆ x= x / ||x||.\n",
    "- Standardize the data by subtracting the mean and dividing by the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_all_train_all_features = matrix_all[:2400] # training set\n",
    "matrix_all_test_all_features = matrix_all[2400:] # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-normalization\n",
    "matrix_all_train_log = np.log(matrix_all_train_all_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 normalization\n",
    "matrix_all_train_normalized1 = preprocessing.normalize(matrix_all_train_all_features, 'l1')\n",
    "matrix_all_test_normalized1 = preprocessing.normalize(matrix_all_test_all_features, 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.2       , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.2       , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.16666667, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.16666667, 0.        , ..., 0.        , 0.        ,\n",
       "        0.16666667],\n",
       "       [0.        , 0.125     , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_all_train_normalized1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 normalization\n",
    "matrix_all_train_normalized2 = preprocessing.normalize(matrix_all_train_all_features, 'l2')\n",
    "matrix_all_test_normalized2 = preprocessing.normalize(matrix_all_test_all_features, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40824829, 0.40824829, 0.40824829, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.4472136 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.4472136 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.40824829, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.40824829, 0.        , ..., 0.        , 0.        ,\n",
       "        0.40824829],\n",
       "       [0.        , 0.31622777, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_all_train_normalized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "matrix_all_train_scaled = preprocessing.scale(matrix_all_train_all_features)\n",
    "matrix_all_test_scaled = preprocessing.scale(matrix_all_test_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.17085051,  0.31700361,  5.45611865, ..., -0.03537746,\n",
       "        -0.03537746, -0.03537746],\n",
       "       [-0.48114316,  0.31700361, -0.18328047, ..., -0.03537746,\n",
       "        -0.03537746, -0.03537746],\n",
       "       [-0.48114316,  0.31700361, -0.18328047, ..., -0.03537746,\n",
       "        -0.03537746, -0.03537746],\n",
       "       ...,\n",
       "       [-0.48114316,  0.31700361, -0.18328047, ..., -0.03537746,\n",
       "        -0.03537746, -0.03537746],\n",
       "       [-0.48114316,  0.31700361, -0.18328047, ..., -0.03537746,\n",
       "        -0.03537746, 28.26658805],\n",
       "       [-0.48114316,  0.31700361, -0.18328047, ..., -0.03537746,\n",
       "        -0.03537746, -0.03537746]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_all_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.77156117e-18,  3.25665421e-17,  5.03301104e-17, ...,\n",
       "        5.92118946e-18,  5.92118946e-18,  5.92118946e-18])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_all_train_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_all_train_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "L1 normalization is considered good for slack matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Sentiment prediction. Train a naive Bayes model on the training set and test on the testing set. Report the classiﬁcation accuracy and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiva_Bayers(X_train, X_test, Y_train):\n",
    "    Y_union_size = 2\n",
    "    sample_size, feature_num = X_train.shape # rows, columns\n",
    "    ps = np.zeros([Y_union_size, feature_num])\n",
    "    ph = np.zeros([Y_union_size])    \n",
    "    for i in range(Y_union_size):\n",
    "        x_i = X_train[Y_train == i]\n",
    "        ps[i] = np.mean(x_i, axis=0)\n",
    "        ph[i] = x_i.shape[0]/float(sample_size) # how many y = i\n",
    "    \n",
    "    sample_size, feature_num = X_test.shape # rows, columns\n",
    "    ps = ps.reshape(Y_union_size, 1, feature_num)\n",
    "    ps = ps.clip(1e-32, 1-1e-32) # avoid log 0\n",
    "    logp_y1 = np.log(ph).reshape([Y_union_size, 1])    \n",
    "    logp_y2 = X_test * np.log(ps) + (1-X_test) * np.log(1-ps)\n",
    "    logp_y = logp_y1 + logp_y2.sum(axis = 2)\n",
    "    Y_pred = logp_y.argmax(axis = 0).flatten()\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = matrix_all_train_normalized1\n",
    "X_test = matrix_all_test_normalized1\n",
    "Y_train = df_all_train['scores']\n",
    "Y_test = df_all_test['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.765\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>255</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>96</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred 0  pred 1\n",
       "true 0     255      45\n",
       "true 1      96     204"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_pred = naiva_Bayers(X_train, X_test, Y_train)\n",
    "\n",
    "# average accuracy\n",
    "score = sklearn.metrics.accuracy_score(score_pred, Y_test)\n",
    "print('accuracy_score =', score)\n",
    "\n",
    "# # confusion matrix\n",
    "cf = sklearn.metrics.confusion_matrix(Y_test, score_pred)\n",
    "pandas.DataFrame(cf, ('true %i'%x for x in range(2)), ('pred %i'%x for x in range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Logistic regression. Now repeat using logistic regression classiﬁcation, and compare performance (you can use existing packages here). Try using both L2 (ridge) regularization and L1 (lasso) regularization and report how these affect the classiﬁcation accuracy and the coefﬁcient vectors (hint: sklearn has a method called LogisticRegressionCV; also note that sklearn doesn’t actually have an implementation of unregularized logisticregression). Inspecting the coefﬁcient vectors, what are the words that play the most important roles in deciding the sentiment of the reviews? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Logistic(X_train, X_test, Y_train):\n",
    "    regr = linear_model.LogisticRegressionCV()\n",
    "    # regr = linear_model.LogisticRegression(C=100)\n",
    "    regr.fit(X_train, Y_train)\n",
    "    Y_pred = regr.predict(X_test)\n",
    "    \n",
    "    return Y_pred, regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def most_impacted_coef(coef):\n",
    "    # get coef with most impact\n",
    "    max_coef = np.max(coef)\n",
    "    max_coef_index = np.where(coef==max_coef)[1][0]\n",
    "    column_name = df_all_words.columns.values[max_coef_index]\n",
    "    print(\"max coef: %.2f, column name (word): %s, frequency: %d\"\n",
    "        % (max_coef, column_name, sum(df_all_words[column_name])))\n",
    "\n",
    "    min_coef = np.min(coef)\n",
    "    min_coef_index = np.where(coef==min_coef)[1][0]\n",
    "    column_name = df_all_words.columns.values[min_coef_index]\n",
    "    print(\"min coef: %.2f, column name (word): %s, frequency: %d\"\n",
    "        % (min_coef, column_name, sum(df_all_words[column_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_result(X_train, X_test, Y_train, Y_test):\n",
    "    Y_pred, coef = train_Logistic(X_train, X_test, Y_train)\n",
    "\n",
    "    # average accuracy\n",
    "    score = sklearn.metrics.accuracy_score(Y_pred, Y_test)\n",
    "    print('accuracy_score =', score)\n",
    "    \n",
    "    # most impacted coef\n",
    "    most_impacted_coef(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.835\n",
      "max coef: 52.98, column name (word): best, frequency: 69\n",
      "min coef: -51.16, column name (word): not, frequency: 366\n"
     ]
    }
   ],
   "source": [
    "Y_train = df_all_train['scores']\n",
    "Y_test = df_all_test['scores']\n",
    "\n",
    "# L1\n",
    "X_train = matrix_all_train_normalized1\n",
    "X_test = matrix_all_test_normalized1\n",
    "Logistic_result(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.83\n",
      "max coef: 16.80, column name (word): best, frequency: 69\n",
      "min coef: -17.15, column name (word): not, frequency: 366\n"
     ]
    }
   ],
   "source": [
    "# L2\n",
    "X_train = matrix_all_train_normalized2\n",
    "X_test = matrix_all_test_normalized2\n",
    "Logistic_result(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- For L1:\n",
    "    - accuracy_score = 0.835\n",
    "    - max coef: 52.98, column name (word): best, frequency: 69\n",
    "    - min coef: -51.16, column name (word): not, frequency: 366\n",
    "\n",
    "- For L2:\n",
    "    - accuracy_score = 0.83\n",
    "    - max coef: 16.80, column name (word): best, frequency: 69\n",
    "    - min coef: -17.15, column name (word): not, frequency: 366\n",
    "\n",
    "The accuracy does not differ significantly with L1 and L2. Though the coeffcients are not the same, the most important words in deciding the sentiment of the reviews are \"best\" and \"not\" for both algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h) N-gram model. Similar to the bag of words model, but now you build up a dictionary of n-grams, which are contiguous sequences of words. For example, “Alice fell down the rabbit hole” would then map to the 2-grams sequence: [\"Alice fell\", \"fell down\", \"down the\", \"the rabbit\",\"rabbit hole\"], and all ﬁve of those symbols would be members of the n-gram dictionary. Try n=2, repeat (d)-(g) and report your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemed_2_words_df(df):\n",
    "    df_words = pandas.DataFrame()\n",
    "\n",
    "    previous_word = ''\n",
    "    for sentence in df['sentences']:\n",
    "        token_dict = {}\n",
    "\n",
    "        token_words=word_tokenize(sentence)\n",
    "        for word in token_words:\n",
    "            stemed = porter.stem(word)\n",
    "            if previous_word != '':\n",
    "                token_dict[previous_word + stemed] = token_dict.get(stemed, 0) + 1\n",
    "            previous_word = stemed + ' '\n",
    "    #     print(token_dict)\n",
    "\n",
    "        df_words = df_words.append(token_dict, ignore_index=True)\n",
    "        \n",
    "    return df_words.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of training data with words as features\n",
    "df_all_train_words = get_stemed_2_words_df(df_all_train)\n",
    "\n",
    "# df of test data with words as features\n",
    "df_all_test_words = get_stemed_2_words_df(df_all_test)\n",
    "\n",
    "# exclude the columns of features not includeed in training data\n",
    "to_drop = []\n",
    "for col in df_all_test_words.columns.values.tolist():\n",
    "    if col not in df_all_train_words.columns.values.tolist():\n",
    "        to_drop.append(col)\n",
    "\n",
    "df_all_test_words = df_all_test_words.drop(columns = to_drop)\n",
    "\n",
    "# df of all data with words as features, combine training and test data\n",
    "df_all_words = df_all_train_words.append(df_all_test_words).fillna(0)\n",
    "matrix_all = np.array(df_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 5961)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_all_train_all_features = matrix_all[:2400]\n",
    "matrix_all_test_all_features = matrix_all[2400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 normalization\n",
    "matrix_all_train_normalized1 = preprocessing.normalize(matrix_all_train_all_features, 'l1')\n",
    "matrix_all_test_normalized1 = preprocessing.normalize(matrix_all_test_all_features, 'l1')\n",
    "\n",
    "# l2 normalization\n",
    "matrix_all_train_normalized2 = preprocessing.normalize(matrix_all_train_all_features, 'l2')\n",
    "matrix_all_test_normalized2 = preprocessing.normalize(matrix_all_test_all_features, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df_all_train['scores']\n",
    "Y_test = df_all_test['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>240</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>48</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred 0  pred 1\n",
       "true 0     240      60\n",
       "true 1      48     252"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1\n",
    "X_train = matrix_all_train_normalized1\n",
    "X_test = matrix_all_test_normalized1\n",
    "\n",
    "# Bayes\n",
    "score_pred = naiva_Bayers(X_train, X_test, Y_train)\n",
    "\n",
    "# average accuracy\n",
    "score = sklearn.metrics.accuracy_score(score_pred, Y_test)\n",
    "print(score)\n",
    "\n",
    "# confusion matrix\n",
    "cf = sklearn.metrics.confusion_matrix(Y_test, score_pred)\n",
    "pandas.DataFrame(cf, ('true %i'%x for x in range(2)), ('pred %i'%x for x in range(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.75\n",
      "max coef: 11.81, column name (word): . great, frequency: 78\n",
      "min coef: -8.23, column name (word): not work, frequency: 42\n"
     ]
    }
   ],
   "source": [
    "# L1\n",
    "# Logistic Regression\n",
    "Logistic_result(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>225</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>45</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred 0  pred 1\n",
       "true 0     225      75\n",
       "true 1      45     255"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2\n",
    "X_train = matrix_all_train_normalized2\n",
    "X_test = matrix_all_test_normalized2\n",
    "\n",
    "# Bayes\n",
    "score_pred = naiva_Bayers(X_train, X_test, Y_train)\n",
    "\n",
    "# average accuracy\n",
    "score = sklearn.metrics.accuracy_score(score_pred, Y_test)\n",
    "print(score)\n",
    "\n",
    "# confusion matrix\n",
    "cf = sklearn.metrics.confusion_matrix(Y_test, score_pred)\n",
    "pandas.DataFrame(cf, ('true %i'%x for x in range(2)), ('pred %i'%x for x in range(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.745\n",
      "max coef: 4.80, column name (word): . great, frequency: 78\n",
      "min coef: -2.91, column name (word): not work, frequency: 42\n"
     ]
    }
   ],
   "source": [
    "# L2\n",
    "Logistic_result(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- for accracy: \n",
    "  - L1:\n",
    "    - Bayes: accuracy_score = 0.82, logistic regression: 0.75\n",
    "  - L2:\n",
    "    - Bayes: accuracy_score = 0.8, logistic regression: 0.745\n",
    "- for coef of Logistic Regression:\n",
    "    - L1:\n",
    "        - max coef: 11.81, column name (word): . great, frequency: 78\n",
    "        - min coef: -8.23, column name (word): not work, frequency: 42\n",
    "    - L2:\n",
    "        - max coef: 4.80, column name (word): . great, frequency: 78\n",
    "        - min coef: -2.91, column name (word): not work, frequency: 42\n",
    "    - Though the coeffcients are not the same, the most important words are \". great\" and \"not work\" for both algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Algorithms comparison and analysis. According to the above results, compare the performances of naive Bayes, logistic regression, naive Bayes with 2-grams, and logistic regression with 2-grams. Which method performs best in the prediction task and why? What do you learn about the language that people use in online reviews (e.g., expressions that will make the posts positive/negative)? Hint: Inspect the weights learned from logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy wise, for 1-gram, logistic regression has higher accuracy scores. For 2-grams, native Bayes has higher accuracy scores. The accuracy difference for using different algorithms is not significate. \n",
    "2-grams keeps more context closer to the original sentences than 1-gram does, which is why native Bayers works better in 2-grams.\n",
    "But logistic regression takes a significate longer time to run than naive Bayes, which is not ideal in practice.\n",
    "\n",
    "In 1-gram and 2-grams, the maximun and minmum coefficients of L1's logistic regression are larger than the L2's. This means L1 highlights the key features better than L2 does.\n",
    "1-gram gives 'best' and 'not' as most impactful words. 2-grams gives \". great\" and \"not work\" as most impactful words. Both of the results include obviously positive and negative words, which makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
